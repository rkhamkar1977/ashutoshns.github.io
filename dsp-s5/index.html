<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"> 
		<link href="https://fonts.googleapis.com/css?family=EB+Garamond" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet"> 
		<style type="text/css">
			body{font-family: 'Raleway', serif;}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-style: normal;font-variant: normal;font-weight: 800;font-size: 36px;padding: 1%;}
			.section{position: relative;width: 90%;clear: left;margin: auto;padding: 2%;}
			.subsection{position: relative;clear: left; width: 90%;text-align: justify;padding: 10px;}
			.heading{position: relative; width: 98%;text-align: left;font-style: normal;font-variant: normal;font-size: 24px;font-weight: 500;}
			.headingss{position: relative; width: 98%;text-align: left;font-weight: 500;font-style: bold;font-size: 20px;font-weight: medium;}
			.text{width: 95%;font-size: 17px;font-style: thin;font-weight: 10;font-variant: normal;text-align: justify;padding: 10px 0px 10px 0px;line-height: 1.5}
			.authors{position: relative;width: 90%;margin: auto;padding: 2%;font-style: thin;font-variant: normal;text-align: center;font-size: 13px;}
			.image{width: 90%;font-size: 15px;text-align: center;}
		</style>
	</head>
	<body>
		<div class="container">
			
			<div class="title">URBAN ENVIRONMENT AUDIO CLASSIFICATION</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>Ashutosh Agrawal, Roll No.: 150102007, Branch: ECE</p>; &nbsp; &nbsp;
				<p> Ashutosh Sharma, Roll No.: 150102008, Branch: ECE</p>; &nbsp; &nbsp;<br>
				<p>   Rahul Khamkar, Roll No.: 150102027, Branch: ECE</p>; &nbsp;&nbsp; 
				<p> Anubhav Agrawal, Roll No.: 150108004, Branch: EEE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->
				
			</div>
			<hr>
			
			<div class="section">
				<div class="heading">Abstract</div>
				<div class="text container">

					<!-- Start edit here  -->
					The goal of this project is to build an audio classifier capable of recognising different sounds. The sounds we have used are ambient noises from an urban environment. Right now we have focused on 4 categories of sounds. This can prove to be a very effective tool for audio surviellance of the sorrounding. It's applications can vary from as simple as making a device to detect a dog's bark to as big as detection of gun shots/bombing or other terrorist activities. We tried plotting different types of spectrograms of samples from each class which inturn lead us to see what filters would be best suited for this classification. In the end we used Mel-frequency cepstral coefficients, Mel-scaled power spectrogram, Chromagram of a short-time Fourier transform, Octave-based spectral contrast, Tonnetz to extract features. The coeffecients from these filters led to 193 features which were then used for classification using Machine Learning.					
					<!-- Stop edit here -->

				</div>
			</div>
			
			<div class="section">
				<div class="heading">1. Introduction</div>
				<div class="text container">

					<!-- Start edit here  -->
					As mentioned, the goal of this project is to build an audio classifier capable of recognising different sounds that are heard in our environment. In this section, I would introduce you to the problem statement and the try to explain the challenges it posed, then present some litreature survey that we did and lastly explaining the approach that we decieded to take for solving this problem.
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="headingss">1.1 Introduction to Problem</div>
					<div class="text container">

						<!-- Start edit here  -->
						The crux of the problem is this: given a never-before-heard recording, how can a system be trained to identify what it is actually listening to?</div>
						<div class="text container">Let’s try to refine this problem. The input format will be the digital representation of the original analogue waveforms, just as would be recorded by a microphone, encoded using pulse-code modulation and stored as (lossless) WAV files.</div>
						<div class="text container">From this input, we’ll need to extract features. But sound recordings are not spreadsheets, with their data neatly organised into rows and columns of known significance. Say we sample two recordings of equal duration, creating 2000 discrete floating point numbers; we can not meaningfully compare the 1000th number of one recording with the 1000th number of another. This is because we have no way of telling whether the number at any particular point in time is signal, silence or noise. Instead, we must abstract away from individual numbers, and consider data values in the context of many of its neighbours. This will grant us the ability to start spotting patterns which we call features, and permit us to generalise - the fundamental requirement in machine learning.</div>
						<div class="text container">The challenge then, is to find measurable properties that differ in dissimilar recordings and are alike in those from similar sources. The complexity of feature extraction makes this problem an attractive application of deep learning, whose hierarchical nature makes it capable of automated feature learning. Once features have somehow been extracted, the question becomes how can we use them to train a model, and then use what we’ve learned to generate predictions?</div>
						<div class="text container">Next, we’ll want to tune the model produced, to achieve the best possible accuracy. So we’ll need to consider how we can measure successful classification - and how can the performance of the model be optimised. Putting all this together suggests a processing pipeline as shown in Fig 1.
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="headingss">1.2 Figure</div>
					<div class="image container">

						<!-- Start edit here  -->
						<div style="width: 85%; float: left;">
							<p>
								<img src="img/fig1.jpg" alt="Overview of the project" width="100%" align="middle"/>
								Fig. 1: An overview block diagram of the project.
							</p>
						</div>
						<div style="width:45%; float:left">
							<p>&nbsp;&nbsp;&nbsp;&nbsp;
								<img src="img/fig2.jpg" alt="Amplitude vs. Time plot for sample files from all 4 categories." style="width:90%; height:auto; align:center;">
								Fig 2: Amplitude vs. Time plot for sample files from all four categories.
							</p>
						</div>
						<div style="width:45%; float:left">
							<p>&nbsp;&nbsp;&nbsp;&nbsp;
								<img src="img/fig3.jpg" alt="Spectrogram of sample files from all 4 categories." style="width:90%; height:auto; align:center;">
								Fig 3: Spectrogram of sample files from all four categories.
							</p>
						</div>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">1.4 Proposed Approach Idea</div>
					<div class="text">

						<!-- Start edit here  -->
						Here is the crude basic idea:
						<ul>
							<li>Using different frequency scales, explore spectrograms.</li>
							<li>Using this, determine what filter banks are ideal for the purpose</li>
							<li>Use these filter banks to generate features</li>
							<li>Use algorithmic training and these features to automate classifier design</li>
							<li>Done. Now test this on new data to calculate efficiency.</li>
						</ul>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">1.5 Feature extraction </div>
					<div class="text container">

						The classification model will be fed with features extracted from our audio dataset, based on which it will be trained to make a decision.All of the audio clips collected have different sampling rates. The librosa library that we used uniformly samples the clips at a frequency of 22KHz. This means that in the time domain, each second of each audio clip input has 22,000 data points. We cannot directly feed this data to train our model, because there may be many redundant features here which will harm our model's accuracy.
					<div class="text container">So we extract typical frequency domain features from these clips. We selected the features to be used here based on the results of similar studies published in the past (check the literature review section). The features we used are-
					<ul>
					<li><a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-frequency cepstral coefficients (MFCC)</a> - the coefficients that collectively make up the short-term power spectrum of a sound</li>
					<li><a href="https://en.wikipedia.org/wiki/Mel_scale">Mel-scaled</a> power spectrogram - the Mel Scale is used to provide greater resolution for more informative (lower) frequencies</li>
					<li></a href="https://labrosa.ee.columbia.edu/matlab/chroma-ansyn/">Chromagram</a> of a short-time Fourier transform - projects into bins representing the 12 distinct semitones (or chroma) of the musical octave</li>
					<li><a href="http://ieeexplore.ieee.org/document/1035731/?reload=true">Octave-based spectral contrast</a> - distributions of sound energy over octave frequencies
					</li>
					<li><a href="https://sites.google.com/site/tonalintervalspace/">Tonnetz</a> - estimates tonal centroids as coordinates in a six-dimensional interval space</li>
					</ul>
					The results of these 5 feature extractions are concatenated to give us a 193-dimensional vector for each audio clip irrespective of its length.</div>
					<div class="subsubsection">
						<div class="headingss">Chromagram of Short time fourier transform</div>
						<div class="text container">
							Short term fourier transform is a very effective and generic tool that can be used to analyze a signal. In normal Fourier
							Tranform we take the entire time domain representation of a signal and observe its frequency domain representation. This tells us
							what frequency components are present in the signal. But in practical cases we often get signals that are largely varying over time. For
							such signals, we not only need to understand what frequency componenets are present in the signla, but how these frequency components are 
							distributed over time for effective analysis. Thus we essentially need a mapping in frequency as well as time domain. In such a situation we break down the signal 
							into small overlapping time frames, using a window function. By varying the window width we can ensure that the signal isnt varying heavily within it.
							Then we take a finite point fourier transform for the data points in these time frames. Thus, we get fourier domain representations for short durations
							of time.
							

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="headingss">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="headingss">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>
			
			
			
	</body>
</html>  
